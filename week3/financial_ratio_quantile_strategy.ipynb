{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Ratio Quantile Trading Strategy Analysis\n",
    "\n",
    "**Course:** Quantitative Trading Strategies  \n",
    "**Assignment:** Week 3 - Financial Ratio Quantiles  \n",
    "**Period:** January 2018 - June 2023  \n",
    "**Universe:** ~1200 US Equities  \n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook implements a quantamental trading strategy based on financial accounting ratios:\n",
    "- **Debt-to-Market-Cap**: Leverage indicator\n",
    "- **Return on Investment (ROI)**: Operating efficiency\n",
    "- **Price-to-Earnings (P/E)**: Valuation metric\n",
    "\n",
    "The strategy constructs long-short portfolios by ranking stocks on these fundamental signals, going long the most attractive decile and short the least attractive decile.\n",
    "\n",
    "**Key Implementation Features:**\n",
    "- Filing-date-aware ratio computation (no look-ahead bias)\n",
    "- Daily market cap adjustments between filings\n",
    "- Multiple signal combination methods (weighted avg, PCA, rank-based)\n",
    "- Position sizing variations (vigintile doubling/halving)\n",
    "- Ratio changes vs absolute values analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "NumPy version: 2.3.5\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Style Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Plot style configured\n"
     ]
    }
   ],
   "source": [
    "def setup_plot_style():\n",
    "    \"\"\"Set up consistent plot styling.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams['figure.figsize'] = (14, 6)\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    plt.rcParams['axes.titlesize'] = 12\n",
    "    plt.rcParams['axes.labelsize'] = 10\n",
    "    plt.rcParams['lines.linewidth'] = 1.5\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "setup_plot_style()\n",
    "print(\"✓ Plot style configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Data directory: data\n",
      "  Analysis period: 2018-01-01 to 2023-06-30\n",
      "  Rebalancing: M\n",
      "  Long/Short quantiles: 0.9/0.1\n",
      "  Min market cap: $100MM\n",
      "  Excluded sectors: ['Automotive', 'Finance', 'Insurance']\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "DATA_DIR = Path('data')\n",
    "OUTPUT_DIR = Path('outputs')\n",
    "PLOTS_DIR = OUTPUT_DIR / 'plots'\n",
    "RESULTS_DIR = OUTPUT_DIR / 'results'\n",
    "\n",
    "# Create directories\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Date range (assignment specification)\n",
    "START_DATE = '2018-01-01'\n",
    "END_DATE = '2023-06-30'\n",
    "\n",
    "# Universe filters (Section 3 of assignment)\n",
    "MIN_MARKET_CAP_MM = 100  # $100MM minimum\n",
    "MIN_DEBT_RATIO = 0.1     # Must exceed 0.1 somewhere in period\n",
    "MIN_DEBT_RATIO_COUNT = 3 # \"More than fleetingly\" = at least 3 quarters\n",
    "\n",
    "# Excluded sectors (assignment Section 3)\n",
    "EXCLUDED_SECTORS = ['Automotive', 'Finance', 'Insurance']\n",
    "\n",
    "# Strategy parameters\n",
    "REBALANCE_FREQ = 'M'  # 'W' for weekly, 'M' for monthly\n",
    "LONG_QUANTILE = 0.90  # Top decile (90th percentile and above)\n",
    "SHORT_QUANTILE = 0.10 # Bottom decile (10th percentile and below)\n",
    "\n",
    "# Capital management (assignment Section 5)\n",
    "LEVERAGE_MULTIPLE = 10  # Initial capital = 10x gross notional\n",
    "FUNDING_RATE = 0.02     # 2% annual (constant) or use rolling LIBOR/SOFR\n",
    "REPO_SPREAD = 0.01      # Repo rate = funding rate - 100bp\n",
    "\n",
    "# Display configuration\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Analysis period: {START_DATE} to {END_DATE}\")\n",
    "print(f\"  Rebalancing: {REBALANCE_FREQ}\")\n",
    "print(f\"  Long/Short quantiles: {LONG_QUANTILE}/{SHORT_QUANTILE}\")\n",
    "print(f\"  Min market cap: ${MIN_MARKET_CAP_MM}MM\")\n",
    "print(f\"  Excluded sectors: {EXCLUDED_SECTORS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Data Classes (Adapted from Week2)\n",
    "\n",
    "These classes are embedded from `week2/src/crypto_spread/strategy.py` and adapted for equity long-short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Core data classes defined\n"
     ]
    }
   ],
   "source": [
    "class PositionSide(Enum):\n",
    "    \"\"\"Position side enumeration.\"\"\"\n",
    "    FLAT = \"FLAT\"\n",
    "    LONG = \"LONG\"\n",
    "    SHORT = \"SHORT\"\n",
    "\n",
    "\n",
    "class ExitReason(Enum):\n",
    "    \"\"\"Exit reason enumeration.\"\"\"\n",
    "    NONE = \"NONE\"\n",
    "    REBALANCE = \"REBALANCE\"  # Monthly/weekly rebalancing\n",
    "    END_OF_DATA = \"END_OF_DATA\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Position:\n",
    "    \"\"\"Represents a trading position in a single stock.\"\"\"\n",
    "    ticker: str\n",
    "    side: PositionSide\n",
    "    entry_price: float\n",
    "    entry_time: pd.Timestamp\n",
    "    shares: float  # Number of shares (can be fractional)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.side == PositionSide.FLAT:\n",
    "            raise ValueError(\"Cannot create a FLAT position object\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Trade:\n",
    "    \"\"\"Represents a completed trade.\"\"\"\n",
    "    ticker: str\n",
    "    side: PositionSide\n",
    "    entry_time: pd.Timestamp\n",
    "    exit_time: pd.Timestamp\n",
    "    entry_price: float\n",
    "    exit_price: float\n",
    "    shares: float\n",
    "    pnl: float\n",
    "    exit_reason: ExitReason\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BacktestResult:\n",
    "    \"\"\"Container for backtest results.\"\"\"\n",
    "    trades: List[Trade]\n",
    "    equity_curve: pd.Series\n",
    "    final_capital: float\n",
    "    total_return: float\n",
    "    sharpe_ratio: float\n",
    "    max_drawdown: float\n",
    "    win_rate: float\n",
    "    num_trades: int\n",
    "    num_rebalances: int\n",
    "    params: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "print(\"✓ Core data classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Metrics Functions (From Week2)\n",
    "\n",
    "Embedded from `week2/src/crypto_spread/metrics.py` with adaptations for daily equity returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Performance metrics functions defined\n"
     ]
    }
   ],
   "source": [
    "def calculate_sharpe_ratio(\n",
    "    equity_curve: pd.Series,\n",
    "    risk_free_rate: float = 0.0,\n",
    "    periods_per_year: int = 252,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate annualized Sharpe ratio using daily returns.\n",
    "    \n",
    "    Formula: Sharpe = sqrt(252) * mean(excess_returns) / std(excess_returns)\n",
    "    \n",
    "    Args:\n",
    "        equity_curve: Series of equity values with datetime index\n",
    "        risk_free_rate: Annual risk-free rate (default: 0%)\n",
    "        periods_per_year: Trading days per year (252)\n",
    "    \n",
    "    Returns:\n",
    "        Annualized Sharpe ratio\n",
    "    \"\"\"\n",
    "    if equity_curve.empty or len(equity_curve) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate daily returns\n",
    "    daily_returns = equity_curve.pct_change().dropna()\n",
    "    \n",
    "    if len(daily_returns) < 1:\n",
    "        return 0.0\n",
    "    \n",
    "    # Excess returns\n",
    "    rf_daily = risk_free_rate / periods_per_year\n",
    "    excess_returns = daily_returns - rf_daily\n",
    "    \n",
    "    mean_excess = excess_returns.mean()\n",
    "    std_excess = excess_returns.std()\n",
    "    \n",
    "    if std_excess == 0 or np.isnan(std_excess):\n",
    "        return 0.0\n",
    "    \n",
    "    sharpe = np.sqrt(periods_per_year) * mean_excess / std_excess\n",
    "    return float(sharpe)\n",
    "\n",
    "\n",
    "def calculate_max_drawdown(equity_curve: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate maximum drawdown as a percentage.\n",
    "    \n",
    "    Args:\n",
    "        equity_curve: Series of equity values over time\n",
    "    \n",
    "    Returns:\n",
    "        Maximum drawdown as a decimal (e.g., 0.10 for 10% drawdown)\n",
    "    \"\"\"\n",
    "    if equity_curve.empty or len(equity_curve) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    running_max = equity_curve.cummax()\n",
    "    drawdown = (equity_curve - running_max) / running_max\n",
    "    max_dd = drawdown.min()\n",
    "    \n",
    "    return abs(float(max_dd))\n",
    "\n",
    "\n",
    "def calculate_win_rate(trades: List[Trade]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate win rate (percentage of profitable trades).\n",
    "    \n",
    "    Args:\n",
    "        trades: List of completed trades\n",
    "    \n",
    "    Returns:\n",
    "        Win rate as a decimal (e.g., 0.60 for 60%)\n",
    "    \"\"\"\n",
    "    if not trades:\n",
    "        return 0.0\n",
    "    \n",
    "    profitable = sum(1 for t in trades if t.pnl > 0)\n",
    "    return profitable / len(trades)\n",
    "\n",
    "\n",
    "def calculate_calmar_ratio(total_return: float, max_drawdown: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Calmar ratio (annualized return / max drawdown).\n",
    "    \n",
    "    Args:\n",
    "        total_return: Total return as decimal\n",
    "        max_drawdown: Maximum drawdown as decimal\n",
    "    \n",
    "    Returns:\n",
    "        Calmar ratio\n",
    "    \"\"\"\n",
    "    if max_drawdown == 0:\n",
    "        return float('inf') if total_return > 0 else 0.0\n",
    "    \n",
    "    return total_return / max_drawdown\n",
    "\n",
    "\n",
    "def calculate_downside_beta(\n",
    "    portfolio_returns: pd.Series,\n",
    "    market_returns: pd.Series,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate downside beta (sensitivity during negative market returns).\n",
    "    \n",
    "    Args:\n",
    "        portfolio_returns: Strategy returns (aligned with market)\n",
    "        market_returns: Market benchmark returns (e.g., S&P 500)\n",
    "    \n",
    "    Returns:\n",
    "        Downside beta\n",
    "    \"\"\"\n",
    "    # Align series\n",
    "    aligned = pd.DataFrame({\n",
    "        'portfolio': portfolio_returns,\n",
    "        'market': market_returns\n",
    "    }).dropna()\n",
    "    \n",
    "    if len(aligned) < 10:\n",
    "        return 0.0\n",
    "    \n",
    "    # Filter to negative market days\n",
    "    negative_days = aligned[aligned['market'] < 0]\n",
    "    \n",
    "    if len(negative_days) < 5:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute covariance and variance\n",
    "    cov = negative_days['portfolio'].cov(negative_days['market'])\n",
    "    var = negative_days['market'].var()\n",
    "    \n",
    "    if var == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return cov / var\n",
    "\n",
    "\n",
    "def calculate_tail_risk(returns: pd.Series, percentile: float = 0.01) -> float:\n",
    "    \"\"\"\n",
    "    Calculate tail risk (Value at Risk at given percentile).\n",
    "    \n",
    "    Args:\n",
    "        returns: Return series\n",
    "        percentile: Percentile for VaR (0.01 for 1%, 0.05 for 5%)\n",
    "    \n",
    "    Returns:\n",
    "        VaR (negative value representing loss)\n",
    "    \"\"\"\n",
    "    if len(returns) < 10:\n",
    "        return 0.0\n",
    "    \n",
    "    return returns.quantile(percentile)\n",
    "\n",
    "\n",
    "print(\"✓ Performance metrics functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_zacks_data() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load all ZACKS fundamental data files.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping table name to DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Loading ZACKS data files...\")\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Financial Condition (balance sheet, income statement, cash flow)\n",
    "    fc_file = list(DATA_DIR.glob('ZACKS_FC_2_*.csv'))[0]\n",
    "    print(f\"  Loading {fc_file.name}...\")\n",
    "    data['fc'] = pd.read_csv(fc_file, parse_dates=['per_end_date', 'filing_date'])\n",
    "    print(f\"    → {len(data['fc']):,} rows\")\n",
    "    \n",
    "    # Financial Ratios (pre-computed)\n",
    "    fr_file = list(DATA_DIR.glob('ZACKS_FR_2_*.csv'))[0]\n",
    "    print(f\"  Loading {fr_file.name}...\")\n",
    "    data['fr'] = pd.read_csv(fr_file, parse_dates=['per_end_date'])  # FR has no filing_date\n",
    "    print(f\"    → {len(data['fr']):,} rows\")\n",
    "    \n",
    "    # Market Value snapshots\n",
    "    mktv_file = list(DATA_DIR.glob('ZACKS_MKTV_2_*.csv'))[0]\n",
    "    print(f\"  Loading {mktv_file.name}...\")\n",
    "    data['mktv'] = pd.read_csv(mktv_file, parse_dates=['per_end_date'])\n",
    "    print(f\"    → {len(data['mktv']):,} rows\")\n",
    "    \n",
    "    # Shares outstanding\n",
    "    shrs_file = list(DATA_DIR.glob('ZACKS_SHRS_2_*.csv'))[0]\n",
    "    print(f\"  Loading {shrs_file.name}...\")\n",
    "    data['shrs'] = pd.read_csv(shrs_file, parse_dates=['per_end_date'])\n",
    "    print(f\"    → {len(data['shrs']):,} rows\")\n",
    "    \n",
    "    # Master ticker (sector info)\n",
    "    mt_file = list(DATA_DIR.glob('ZACKS_MT_2_*.csv'))[0]\n",
    "    print(f\"  Loading {mt_file.name}...\")\n",
    "    data['mt'] = pd.read_csv(mt_file)\n",
    "    print(f\"    → {len(data['mt']):,} rows\")\n",
    "    \n",
    "    print(\"✓ ZACKS data loaded successfully\\n\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_price_data(tickers: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load QUOTEMEDIA price data.\n",
    "    \n",
    "    Args:\n",
    "        tickers: Optional list of tickers to filter (reduces memory)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: ticker, date, adj_close, adj_volume\n",
    "    \"\"\"\n",
    "    prices_file = list(DATA_DIR.glob('QUOTEMEDIA_PRICES_*.csv'))[0]\n",
    "    print(f\"Loading {prices_file.name} (this may take a moment...)\")\n",
    "    \n",
    "    # Use chunked reading for large file\n",
    "    usecols = ['ticker', 'date', 'adj_close', 'adj_volume']\n",
    "    \n",
    "    if tickers is not None:\n",
    "        # Filter by ticker during load (memory efficient)\n",
    "        ticker_set = set(tickers)\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(prices_file, usecols=usecols, parse_dates=['date'], chunksize=1_000_000):\n",
    "            filtered = chunk[chunk['ticker'].isin(ticker_set)]\n",
    "            chunks.append(filtered)\n",
    "        prices = pd.concat(chunks, ignore_index=True)\n",
    "    else:\n",
    "        prices = pd.read_csv(prices_file, usecols=usecols, parse_dates=['date'])\n",
    "    \n",
    "    print(f\"  → {len(prices):,} price records loaded\")\n",
    "    return prices\n",
    "\n",
    "\n",
    "print(\"✓ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Universe Definition (Section 3 of Assignment)\n",
    "\n",
    "Apply 5 sequential filters to construct the ~1200 ticker universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Universe filter functions defined\n"
     ]
    }
   ],
   "source": [
    "def apply_universe_filters(\n",
    "    zacks_data: Dict[str, pd.DataFrame],\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply 5 universe filters per assignment Section 3.\n",
    "    \n",
    "    Filters:\n",
    "    1. Date coverage: prices available for entire period\n",
    "    2. Market cap: never below $100MM\n",
    "    3. Debt ratio: > 0.1 somewhere (\"more than fleetingly\")\n",
    "    4. Sector: exclude automotive, financial, insurance\n",
    "    5. Ratio feasibility: all 3 ratios computable\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with universe tickers and metadata\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"APPLYING UNIVERSE FILTERS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Convert dates\n",
    "    start_dt = pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Filter 1: Date coverage\n",
    "    print(\"\\n[Filter 1] Date coverage check\")\n",
    "    print(f\"  Loading sample of price data to check coverage...\")\n",
    "    \n",
    "    # Load just ticker and date columns for coverage check\n",
    "    prices_file = list(DATA_DIR.glob('QUOTEMEDIA_PRICES_*.csv'))[0]\n",
    "    prices_sample = pd.read_csv(prices_file, usecols=['ticker', 'date'], parse_dates=['date'])\n",
    "    prices_period = prices_sample[\n",
    "        (prices_sample['date'] >= start_dt) & \n",
    "        (prices_sample['date'] <= end_dt)\n",
    "    ]\n",
    "    \n",
    "    # Count trading days per ticker\n",
    "    total_days = len(prices_period['date'].unique())\n",
    "    ticker_days = prices_period.groupby('ticker')['date'].nunique()\n",
    "    \n",
    "    # Require at least 95% coverage (allow for some holidays/suspensions)\n",
    "    min_days = int(total_days * 0.95)\n",
    "    tickers_with_coverage = ticker_days[ticker_days >= min_days].index.tolist()\n",
    "    \n",
    "    print(f\"  Total trading days in period: {total_days}\")\n",
    "    print(f\"  Required coverage: {min_days} days (95%)\")\n",
    "    print(f\"  ✓ Passed Filter 1: {len(tickers_with_coverage):,} tickers\")\n",
    "    \n",
    "    universe = pd.DataFrame({'ticker': tickers_with_coverage})\n",
    "    \n",
    "    # Filter 2: Market cap minimum\n",
    "    print(\"\\n[Filter 2] Market cap minimum ($100MM)\")\n",
    "    mktv = zacks_data['mktv']\n",
    "    mktv_period = mktv[\n",
    "        (mktv['per_end_date'] >= start_dt) & \n",
    "        (mktv['per_end_date'] <= end_dt)\n",
    "    ]\n",
    "    \n",
    "    # Find minimum market cap per ticker\n",
    "    min_mktv = mktv_period.groupby('ticker')['mkt_val'].min()\n",
    "    tickers_mktv_ok = min_mktv[min_mktv >= MIN_MARKET_CAP_MM].index.tolist()\n",
    "    \n",
    "    universe = universe[universe['ticker'].isin(tickers_mktv_ok)]\n",
    "    print(f\"  ✓ Passed Filter 2: {len(universe):,} tickers\")\n",
    "    \n",
    "    # Filter 3: Debt ratio existence\n",
    "    print(f\"\\n[Filter 3] Debt ratio > {MIN_DEBT_RATIO} (at least {MIN_DEBT_RATIO_COUNT} quarters)\")\n",
    "    fr = zacks_data['fr']\n",
    "    fr_period = fr[\n",
    "        (fr['per_end_date'] >= start_dt) & \n",
    "        (fr['per_end_date'] <= end_dt)\n",
    "    ]\n",
    "    \n",
    "    # Count quarters with debt ratio > threshold\n",
    "    debt_ratio_counts = fr_period[\n",
    "        fr_period['tot_debt_tot_equity'] > MIN_DEBT_RATIO\n",
    "    ].groupby('ticker').size()\n",
    "    \n",
    "    tickers_debt_ok = debt_ratio_counts[debt_ratio_counts >= MIN_DEBT_RATIO_COUNT].index.tolist()\n",
    "    \n",
    "    universe = universe[universe['ticker'].isin(tickers_debt_ok)]\n",
    "    print(f\"  ✓ Passed Filter 3: {len(universe):,} tickers\")\n",
    "    \n",
    "    # Filter 4: Sector exclusions\n",
    "    print(f\"\\n[Filter 4] Sector exclusions: {EXCLUDED_SECTORS}\")\n",
    "    mt = zacks_data['mt']\n",
    "    \n",
    "    # Map sector codes (need to check actual column names)\n",
    "    if 'zacks_sector_code' in mt.columns:\n",
    "        sector_col = 'zacks_sector_code'\n",
    "    elif 'zacks_x_sector_desc' in mt.columns:\n",
    "        sector_col = 'zacks_x_sector_desc'\n",
    "    else:\n",
    "        # Try to find any sector column\n",
    "        sector_cols = [c for c in mt.columns if 'sector' in c.lower()]\n",
    "        sector_col = sector_cols[0] if sector_cols else None\n",
    "    \n",
    "    if sector_col:\n",
    "        excluded_tickers = mt[\n",
    "            mt[sector_col].str.contains('|'.join(EXCLUDED_SECTORS), case=False, na=False)\n",
    "        ]['ticker'].tolist()\n",
    "        \n",
    "        universe = universe[~universe['ticker'].isin(excluded_tickers)]\n",
    "        print(f\"  Excluded {len(excluded_tickers):,} tickers from excluded sectors\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Warning: Could not find sector column, skipping sector filter\")\n",
    "    \n",
    "    print(f\"  ✓ Passed Filter 4: {len(universe):,} tickers\")\n",
    "    \n",
    "    # Filter 5: Ratio feasibility\n",
    "    print(\"\\n[Filter 5] Ratio feasibility (all 3 ratios computable)\")\n",
    "    \n",
    "    # Check for required columns in FC\n",
    "    fc = zacks_data['fc']\n",
    "    fc_period = fc[\n",
    "        (fc['per_end_date'] >= start_dt) & \n",
    "        (fc['per_end_date'] <= end_dt)\n",
    "    ]\n",
    "    \n",
    "    # Tickers with EPS data (for P/E)\n",
    "    tickers_with_eps = fc_period[\n",
    "        fc_period['eps_diluted_net'].notna() | fc_period['basic_net_eps'].notna()\n",
    "    ]['ticker'].unique()\n",
    "    \n",
    "    # Tickers with debt data (for debt ratio and ROI)\n",
    "    tickers_with_debt = fc_period[\n",
    "        fc_period['tot_lterm_debt'].notna() | fc_period['net_lterm_debt'].notna()\n",
    "    ]['ticker'].unique()\n",
    "    \n",
    "    # Tickers with ROI data\n",
    "    tickers_with_roi = fr_period['ret_invst'].notna().groupby(fr_period['ticker']).any()\n",
    "    tickers_with_roi = tickers_with_roi[tickers_with_roi].index.tolist()\n",
    "    \n",
    "    # Intersection: all 3 ratios computable\n",
    "    feasible_tickers = list(\n",
    "        set(tickers_with_eps) & \n",
    "        set(tickers_with_debt) & \n",
    "        set(tickers_with_roi)\n",
    "    )\n",
    "    \n",
    "    universe = universe[universe['ticker'].isin(feasible_tickers)]\n",
    "    print(f\"  ✓ Passed Filter 5: {len(universe):,} tickers\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"FINAL UNIVERSE: {len(universe):,} tickers\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return universe\n",
    "\n",
    "\n",
    "print(\"✓ Universe filter functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Universe Construction\n",
    "\n",
    "Load data and apply filters to build the ~1200 ticker universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ZACKS data files...\n",
      "  Loading ZACKS_FC_2_76e4bece47ce87cb8f221f639c7f829b.csv...\n",
      "    → 649,883 rows\n",
      "  Loading ZACKS_FR_2_f40c6a304f87d9f492c1f21839d474e2.csv...\n",
      "    → 649,883 rows\n",
      "  Loading ZACKS_MKTV_2_ecb7f768974bbdd26964caefe2fd0378.csv...\n",
      "    → 1,058,327 rows\n",
      "  Loading ZACKS_SHRS_2_99db6fa97ac677f3c0d45a9fa9a70196.csv...\n",
      "    → 1,058,399 rows\n",
      "  Loading ZACKS_MT_2_5c2afb6368dcc3ed48e1a84279323e63.csv...\n",
      "    → 38,868 rows\n",
      "✓ ZACKS data loaded successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load ZACKS fundamental data\n",
    "zacks_data = load_zacks_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "APPLYING UNIVERSE FILTERS\n",
      "================================================================================\n",
      "\n",
      "[Filter 1] Date coverage check\n",
      "  Loading sample of price data to check coverage...\n"
     ]
    }
   ],
   "source": [
    "# Apply universe filters\n",
    "universe = apply_universe_filters(zacks_data, START_DATE, END_DATE)\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nUniverse sample:\")\n",
    "print(universe.head(10))\n",
    "\n",
    "# Export universe list\n",
    "universe_file = RESULTS_DIR / 'universe_tickers.csv'\n",
    "universe.to_csv(universe_file, index=False)\n",
    "print(f\"\\n✓ Universe saved to {universe_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Price Data for Universe\n",
    "\n",
    "Now that we have the universe, load only the relevant price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QUOTEMEDIA_PRICES_247f636d651d8ef83d8ca1e756cf5ee4.csv (this may take a moment...)\n",
      "  → 10,530,600 price records loaded\n",
      "\n",
      "Price data shape: (2295752, 4)\n",
      "Date range: 2018-01-02 00:00:00 to 2023-06-30 00:00:00\n",
      "Unique tickers: 1661\n",
      "\n",
      "Price data sample:\n",
      "  ticker       date  adj_close  adj_volume\n",
      "0      A 2018-01-02    64.3125   1047830.0\n",
      "1      A 2018-01-03    65.9488   1698899.0\n",
      "2      A 2018-01-04    65.4541   2231534.0\n",
      "3      A 2018-01-05    66.5006   1632512.0\n",
      "4      A 2018-01-08    66.6433   1613911.0\n",
      "5      A 2018-01-09    68.2797   2666711.0\n",
      "6      A 2018-01-10    67.3473   2957184.0\n",
      "7      A 2018-01-11    67.3569   1511134.0\n",
      "8      A 2018-01-12    68.2416   1448155.0\n",
      "9      A 2018-01-16    67.7659   1703398.0\n"
     ]
    }
   ],
   "source": [
    "# Load prices for universe tickers only (memory efficient)\n",
    "universe_tickers = universe['ticker'].tolist()\n",
    "prices = load_price_data(tickers=universe_tickers)\n",
    "\n",
    "# Filter to date range\n",
    "prices = prices[\n",
    "    (prices['date'] >= START_DATE) & \n",
    "    (prices['date'] <= END_DATE)\n",
    "].sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nPrice data shape: {prices.shape}\")\n",
    "print(f\"Date range: {prices['date'].min()} to {prices['date'].max()}\")\n",
    "print(f\"Unique tickers: {prices['ticker'].nunique()}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nPrice data sample:\")\n",
    "print(prices.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Financial Ratio Computation (Section 4 of Assignment)\n",
    "\n",
    "Implement filing-date-aware ratio computation with daily market cap adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Import Validated Ratio Calculator\n",
    "\n",
    "Import the production-ready `calculate_financial_ratios` function from ratio_calculator.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imported calculate_financial_ratios from ratio_calculator.py\n"
     ]
    }
   ],
   "source": [
    "# Import validated ratio calculator\n",
    "import sys\n",
    "sys.path.insert(0, str(Path('.').resolve()))\n",
    "from ratio_calculator import calculate_financial_ratios\n",
    "\n",
    "print(\"✓ Imported calculate_financial_ratios from ratio_calculator.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Vectorized Daily Ratio Computation\n",
    "\n",
    "Compute all three ratios for the entire universe with daily updates based on price changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ compute_all_ratios function defined\n"
     ]
    }
   ],
   "source": [
    "def compute_all_ratios(\n",
    "    tickers: List[str],\n",
    "    fc: pd.DataFrame,\n",
    "    fr: pd.DataFrame,\n",
    "    mktv: pd.DataFrame,\n",
    "    shrs: pd.DataFrame,\n",
    "    prices: pd.DataFrame,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    progress_interval: int = 100\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute all three financial ratios for every ticker on every trading day.\n",
    "    \n",
    "    Uses filing-date-aware computation: ratios are forward-filled from filing_date\n",
    "    to the next filing_date, with daily price adjustments.\n",
    "    \n",
    "    Args:\n",
    "        tickers: List of universe tickers\n",
    "        fc: Financial Condition data (ZACKS_FC)\n",
    "        fr: Financial Ratios data (ZACKS_FR)\n",
    "        mktv: Market Value data (ZACKS_MKTV)\n",
    "        shrs: Shares Outstanding data (ZACKS_SHRS)\n",
    "        prices: Daily price data (QUOTEMEDIA_PRICES)\n",
    "        start_date: Start of analysis period\n",
    "        end_date: End of analysis period\n",
    "        progress_interval: Print progress every N tickers\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: ticker, date, debt_mktcap, roi, pe\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPUTING FINANCIAL RATIOS FOR ALL TICKERS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    start_dt = pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Preprocess: index fundamental data by ticker for fast lookup\n",
    "    fc_by_ticker = {t: g.sort_values('filing_date') for t, g in fc.groupby('ticker')}\n",
    "    fr_by_ticker = {t: g.sort_values('per_end_date') for t, g in fr.groupby('ticker')}\n",
    "    mktv_by_ticker = {t: g.sort_values('per_end_date') for t, g in mktv.groupby('ticker')}\n",
    "    shrs_by_ticker = {t: g.sort_values('per_end_date') for t, g in shrs.groupby('ticker')}\n",
    "    prices_by_ticker = {t: g.sort_values('date') for t, g in prices.groupby('ticker')}\n",
    "    \n",
    "    all_ratios = []\n",
    "    failed_tickers = []\n",
    "    \n",
    "    for i, ticker in enumerate(tickers):\n",
    "        if (i + 1) % progress_interval == 0:\n",
    "            print(f\"  Processing ticker {i+1}/{len(tickers)}: {ticker}\")\n",
    "        \n",
    "        # Get ticker data\n",
    "        ticker_fc = fc_by_ticker.get(ticker)\n",
    "        ticker_fr = fr_by_ticker.get(ticker)\n",
    "        ticker_mktv = mktv_by_ticker.get(ticker)\n",
    "        ticker_shrs = shrs_by_ticker.get(ticker)\n",
    "        ticker_prices = prices_by_ticker.get(ticker)\n",
    "        \n",
    "        # Skip if missing data\n",
    "        if ticker_fc is None or ticker_fr is None or ticker_mktv is None or ticker_shrs is None or ticker_prices is None:\n",
    "            failed_tickers.append((ticker, \"missing_data\"))\n",
    "            continue\n",
    "        \n",
    "        if len(ticker_fc) == 0 or len(ticker_prices) == 0:\n",
    "            failed_tickers.append((ticker, \"empty_data\"))\n",
    "            continue\n",
    "        \n",
    "        # Filter to date range\n",
    "        ticker_fc = ticker_fc[(ticker_fc['filing_date'] >= start_dt - pd.Timedelta(days=365)) & \n",
    "                               (ticker_fc['filing_date'] <= end_dt)]\n",
    "        ticker_prices = ticker_prices[(ticker_prices['date'] >= start_dt) & \n",
    "                                       (ticker_prices['date'] <= end_dt)]\n",
    "        \n",
    "        if len(ticker_fc) == 0 or len(ticker_prices) == 0:\n",
    "            failed_tickers.append((ticker, \"no_data_in_range\"))\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Build daily ratio series\n",
    "            ticker_ratios = []\n",
    "            \n",
    "            # Get all unique filing dates\n",
    "            filing_dates = ticker_fc['filing_date'].sort_values().unique()\n",
    "            \n",
    "            for j, filing_date in enumerate(filing_dates):\n",
    "                # Get fundamental data for this filing\n",
    "                fc_row = ticker_fc[ticker_fc['filing_date'] == filing_date].iloc[0]\n",
    "                per_end = fc_row['per_end_date']\n",
    "                \n",
    "                # Find matching FR, MKTV, SHRS data\n",
    "                fr_match = ticker_fr[ticker_fr['per_end_date'] == per_end]\n",
    "                mktv_match = ticker_mktv[ticker_mktv['per_end_date'] == per_end]\n",
    "                shrs_match = ticker_shrs[ticker_shrs['per_end_date'] == per_end]\n",
    "                \n",
    "                if len(fr_match) == 0 or len(mktv_match) == 0 or len(shrs_match) == 0:\n",
    "                    continue\n",
    "                \n",
    "                fr_row = fr_match.iloc[0]\n",
    "                mktv_row = mktv_match.iloc[0]\n",
    "                shrs_row = shrs_match.iloc[0]\n",
    "                \n",
    "                # Determine date range this filing applies to\n",
    "                next_filing = filing_dates[j + 1] if j + 1 < len(filing_dates) else end_dt + pd.Timedelta(days=1)\n",
    "                \n",
    "                # Get prices in this range\n",
    "                filing_date_ts = pd.Timestamp(filing_date)\n",
    "                next_filing_ts = pd.Timestamp(next_filing)\n",
    "                \n",
    "                range_prices = ticker_prices[\n",
    "                    (ticker_prices['date'] >= filing_date_ts) & \n",
    "                    (ticker_prices['date'] < next_filing_ts)\n",
    "                ]\n",
    "                \n",
    "                if len(range_prices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Compute ratios for each trading day\n",
    "                for _, price_row in range_prices.iterrows():\n",
    "                    try:\n",
    "                        ratios = calculate_financial_ratios(\n",
    "                            ticker=ticker,\n",
    "                            per_end_date=per_end,\n",
    "                            current_date=price_row['date'],\n",
    "                            fc_data=fc_row,\n",
    "                            fr_data=fr_row,\n",
    "                            mktv_data=mktv_row,\n",
    "                            shrs_data=shrs_row,\n",
    "                            prices_data=ticker_prices\n",
    "                        )\n",
    "                        \n",
    "                        ticker_ratios.append({\n",
    "                            'ticker': ticker,\n",
    "                            'date': price_row['date'],\n",
    "                            'debt_mktcap': ratios['debt_mktcap'],\n",
    "                            'roi': ratios['roi'],\n",
    "                            'pe': ratios['pe'],\n",
    "                            'mkt_cap': ratios['mkt_cap']\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        # Skip days with computation errors\n",
    "                        continue\n",
    "            \n",
    "            if ticker_ratios:\n",
    "                all_ratios.extend(ticker_ratios)\n",
    "        \n",
    "        except Exception as e:\n",
    "            failed_tickers.append((ticker, str(e)[:50]))\n",
    "            continue\n",
    "    \n",
    "    # Combine all ratios\n",
    "    ratios_df = pd.DataFrame(all_ratios)\n",
    "    \n",
    "    print(f\"\\n✓ Computed ratios for {ratios_df['ticker'].nunique():,} tickers\")\n",
    "    print(f\"  Total daily observations: {len(ratios_df):,}\")\n",
    "    print(f\"  Failed tickers: {len(failed_tickers):,}\")\n",
    "    \n",
    "    if failed_tickers[:5]:\n",
    "        print(f\"  Sample failures: {failed_tickers[:5]}\")\n",
    "    \n",
    "    return ratios_df\n",
    "\n",
    "\n",
    "print(\"✓ compute_all_ratios function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPUTING FINANCIAL RATIOS FOR ALL TICKERS\n",
      "================================================================================\n",
      "  Processing ticker 100/1661: AQN\n",
      "  Processing ticker 200/1661: BKNG\n",
      "  Processing ticker 300/1661: CLH\n",
      "  Processing ticker 400/1661: DAL\n",
      "  Processing ticker 500/1661: ESE\n",
      "  Processing ticker 600/1661: GHC\n",
      "  Processing ticker 700/1661: HTHT\n",
      "  Processing ticker 800/1661: KGC\n",
      "  Processing ticker 900/1661: MCFT\n",
      "  Processing ticker 1000/1661: NDSN\n",
      "  Processing ticker 1100/1661: OSG\n",
      "  Processing ticker 1200/1661: PXD\n",
      "  Processing ticker 1300/1661: SENEA\n",
      "  Processing ticker 1400/1661: SXI\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compute ratios for the entire universe\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This cell takes several minutes to run\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m ratios_df = \u001b[43mcompute_all_ratios\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m=\u001b[49m\u001b[43muniverse_tickers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzacks_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzacks_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmktv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzacks_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmktv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzacks_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mshrs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSTART_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEND_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Display sample\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRatios sample:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mcompute_all_ratios\u001b[39m\u001b[34m(tickers, fc, fr, mktv, shrs, prices, start_date, end_date, progress_interval)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, price_row \u001b[38;5;129;01min\u001b[39;00m range_prices.iterrows():\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         ratios = \u001b[43mcalculate_financial_ratios\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m            \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m            \u001b[49m\u001b[43mper_end_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mper_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcurrent_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprice_row\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfc_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfc_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfr_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfr_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmktv_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmktv_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshrs_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshrs_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprices_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mticker_prices\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m         ticker_ratios.append({\n\u001b[32m    133\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m'\u001b[39m: ticker,\n\u001b[32m    134\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m: price_row[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mmkt_cap\u001b[39m\u001b[33m'\u001b[39m: ratios[\u001b[33m'\u001b[39m\u001b[33mmkt_cap\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    139\u001b[39m         })\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    141\u001b[39m         \u001b[38;5;66;03m# Skip days with computation errors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\Github_repo\\qts_project-1\\week3\\ratio_calculator.py:102\u001b[39m, in \u001b[36mcalculate_financial_ratios\u001b[39m\u001b[34m(ticker, per_end_date, current_date, fc_data, fr_data, mktv_data, shrs_data, prices_data)\u001b[39m\n\u001b[32m     99\u001b[39m current_dt = pd.to_datetime(current_date)\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Price at per_end_date (or most recent before)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m mask = prices_data[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] <= per_end_dt\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mask.any():\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo price data available on or before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mper_end_dt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\awang\\Downloads\\Github_repo\\qts_project-1\\week2\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4104\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4102\u001b[39m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m com.is_bool_indexer(key):\n\u001b[32m-> \u001b[39m\u001b[32m4104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4106\u001b[39m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[32m   4107\u001b[39m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[32m   4108\u001b[39m is_single_key = \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\awang\\Downloads\\Github_repo\\qts_project-1\\week2\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4166\u001b[39m, in \u001b[36mDataFrame._getitem_bool_array\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(deep=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   4165\u001b[39m indexer = key.nonzero()[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m4166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\awang\\Downloads\\Github_repo\\qts_project-1\\week2\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4175\u001b[39m, in \u001b[36mNDFrame._take_with_is_copy\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4164\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   4165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis = \u001b[32m0\u001b[39m) -> Self:\n\u001b[32m   4166\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4167\u001b[39m \u001b[33;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[32m   4168\u001b[39m \u001b[33;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4173\u001b[39m \u001b[33;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[32m   4174\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4175\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4176\u001b[39m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[32m   4177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result._get_axis(axis).equals(\u001b[38;5;28mself\u001b[39m._get_axis(axis)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\awang\\Downloads\\Github_repo\\qts_project-1\\week2\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4155\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4150\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4151\u001b[39m     indices = np.arange(\n\u001b[32m   4152\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4153\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4159\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4161\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4162\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\awang\\Downloads\\Github_repo\\qts_project-1\\week2\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:913\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    910\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    912\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m913\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\awang\\Downloads\\Github_repo\\qts_project-1\\week2\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:707\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    699\u001b[39m     new_blocks = \u001b[38;5;28mself\u001b[39m._slice_take_blocks_ax0(\n\u001b[32m    700\u001b[39m         indexer,\n\u001b[32m    701\u001b[39m         fill_value=fill_value,\n\u001b[32m    702\u001b[39m         only_slice=only_slice,\n\u001b[32m    703\u001b[39m         use_na_proxy=use_na_proxy,\n\u001b[32m    704\u001b[39m     )\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    706\u001b[39m     new_blocks = [\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    715\u001b[39m     ]\n\u001b[32m    717\u001b[39m new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m    718\u001b[39m new_axes[axis] = new_axis\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\awang\\Downloads\\Github_repo\\qts_project-1\\week2\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1373\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1370\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1372\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1378\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1380\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1381\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\awang\\Downloads\\Github_repo\\qts_project-1\\week2\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\awang\\Downloads\\Github_repo\\qts_project-1\\week2\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    157\u001b[39m     out = np.empty(out_shape, dtype=dtype)\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[32m    165\u001b[39m     out = out.T\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Compute ratios for the entire universe\n",
    "# This cell takes several minutes to run\n",
    "\n",
    "ratios_df = compute_all_ratios(\n",
    "    tickers=universe_tickers,\n",
    "    fc=zacks_data['fc'],\n",
    "    fr=zacks_data['fr'],\n",
    "    mktv=zacks_data['mktv'],\n",
    "    shrs=zacks_data['shrs'],\n",
    "    prices=prices,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    progress_interval=100\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nRatios sample:\")\n",
    "print(ratios_df.head(10))\n",
    "print(f\"\\nDate range: {ratios_df['date'].min()} to {ratios_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Combined Scoring Method\n",
    "\n",
    "Create a combined signal using z-score normalized ratios with equal weighting.\n",
    "\n",
    "**Signal Interpretation:**\n",
    "- **Debt/MktCap**: Lower is better (less leverage) → invert for ranking\n",
    "- **ROI**: Higher is better (more efficient)\n",
    "- **P/E**: Lower is better (cheaper valuation) → invert for ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_combined_score(ratios_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute combined score using z-score normalization and equal weighting.\n",
    "    \n",
    "    For each date:\n",
    "    1. Z-score normalize each ratio across all tickers\n",
    "    2. Invert signals where lower is better (debt_mktcap, pe)\n",
    "    3. Average the z-scores to get combined score\n",
    "    \n",
    "    Args:\n",
    "        ratios_df: DataFrame with ticker, date, debt_mktcap, roi, pe\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with additional columns: z_debt_mktcap, z_roi, z_pe, combined_score\n",
    "    \"\"\"\n",
    "    print(\"Computing combined scores...\")\n",
    "    \n",
    "    df = ratios_df.copy()\n",
    "    \n",
    "    # Handle infinities and extreme outliers by winsorizing\n",
    "    for col in ['debt_mktcap', 'roi', 'pe']:\n",
    "        # Replace infinities\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        # Winsorize at 1st and 99th percentiles\n",
    "        lower = df[col].quantile(0.01)\n",
    "        upper = df[col].quantile(0.99)\n",
    "        df[col] = df[col].clip(lower, upper)\n",
    "    \n",
    "    # Z-score normalize within each date\n",
    "    def zscore_group(group):\n",
    "        for col in ['debt_mktcap', 'roi', 'pe']:\n",
    "            mean = group[col].mean()\n",
    "            std = group[col].std()\n",
    "            if std > 0:\n",
    "                group[f'z_{col}'] = (group[col] - mean) / std\n",
    "            else:\n",
    "                group[f'z_{col}'] = 0\n",
    "        return group\n",
    "    \n",
    "    df = df.groupby('date', group_keys=False).apply(zscore_group)\n",
    "    \n",
    "    # Invert signals where lower is better\n",
    "    # debt_mktcap: lower is better → negate z-score\n",
    "    # pe: lower is better → negate z-score\n",
    "    # roi: higher is better → keep as is\n",
    "    df['z_debt_mktcap_adj'] = -df['z_debt_mktcap']\n",
    "    df['z_pe_adj'] = -df['z_pe']\n",
    "    df['z_roi_adj'] = df['z_roi']\n",
    "    \n",
    "    # Combined score: equal-weighted average\n",
    "    df['combined_score'] = (df['z_debt_mktcap_adj'] + df['z_roi_adj'] + df['z_pe_adj']) / 3\n",
    "    \n",
    "    print(f\"✓ Combined scores computed for {len(df):,} observations\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Compute combined scores\n",
    "ratios_df = compute_combined_score(ratios_df)\n",
    "\n",
    "print(\"\\nScore summary statistics:\")\n",
    "print(ratios_df[['debt_mktcap', 'roi', 'pe', 'combined_score']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Ratio Changes Computation\n",
    "\n",
    "Per assignment Section 5: Compute ratio changes (Δratio = ratio_t - ratio_{t-1}) as an alternative signal.\n",
    "This tests whether **changes** in fundamentals are more predictive than **levels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ratio_changes(ratios_df: pd.DataFrame, lookback_days: int = 21) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute ratio changes from lookback period (approximately 1 month).\n",
    "    \n",
    "    Δratio = ratio_today - ratio_lookback_days_ago\n",
    "    \n",
    "    Args:\n",
    "        ratios_df: DataFrame with daily ratios\n",
    "        lookback_days: Number of trading days for change calculation\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with additional change columns\n",
    "    \"\"\"\n",
    "    print(f\"Computing ratio changes (lookback={lookback_days} days)...\")\n",
    "    \n",
    "    df = ratios_df.copy().sort_values(['ticker', 'date'])\n",
    "    \n",
    "    # Compute lagged values for each ticker\n",
    "    for col in ['debt_mktcap', 'roi', 'pe', 'combined_score']:\n",
    "        df[f'{col}_lag'] = df.groupby('ticker')[col].shift(lookback_days)\n",
    "        df[f'd_{col}'] = df[col] - df[f'{col}_lag']\n",
    "    \n",
    "    # Drop rows without valid changes (beginning of series)\n",
    "    valid_mask = df['d_debt_mktcap'].notna()\n",
    "    df_changes = df[valid_mask].copy()\n",
    "    \n",
    "    print(f\"✓ Computed changes for {len(df_changes):,} observations\")\n",
    "    print(f\"  Dropped {(~valid_mask).sum():,} observations without lookback data\")\n",
    "    \n",
    "    return df_changes\n",
    "\n",
    "\n",
    "# Compute ratio changes\n",
    "ratios_with_changes = compute_ratio_changes(ratios_df, lookback_days=21)\n",
    "\n",
    "print(\"\\nChange statistics:\")\n",
    "print(ratios_with_changes[['d_debt_mktcap', 'd_roi', 'd_pe', 'd_combined_score']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save computed ratios for later use\n",
    "ratios_file = RESULTS_DIR / 'computed_ratios.parquet'\n",
    "ratios_df.to_parquet(ratios_file, index=False)\n",
    "print(f\"✓ Saved ratios to {ratios_file}\")\n",
    "\n",
    "changes_file = RESULTS_DIR / 'ratio_changes.parquet'\n",
    "ratios_with_changes.to_parquet(changes_file, index=False)\n",
    "print(f\"✓ Saved ratio changes to {changes_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Backtesting Engine\n",
    "\n",
    "Implements the quantile long-short strategy with:\n",
    "- Monthly and weekly rebalancing\n",
    "- Quantile-based portfolio construction (top/bottom decile)\n",
    "- Position sizing variations (base, vigintile doubling, vigintile halving)\n",
    "- Funding cost calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Strategy Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StrategyConfig:\n",
    "    \"\"\"Configuration for a single strategy variant.\"\"\"\n",
    "    name: str\n",
    "    signal_col: str          # Column to rank on (e.g., 'debt_mktcap', 'combined_score', 'd_roi')\n",
    "    rebalance_freq: str      # 'M' for monthly, 'W' for weekly\n",
    "    sizing_method: str       # 'equal', 'vigintile_double', 'vigintile_half'\n",
    "    long_quantile: float     # Percentile threshold for long (e.g., 0.90)\n",
    "    short_quantile: float    # Percentile threshold for short (e.g., 0.10)\n",
    "    invert_signal: bool      # True if lower signal value is better (for long)\n",
    "    use_changes: bool = False  # True to use ratio changes instead of levels\n",
    "\n",
    "\n",
    "# Define all 24+ strategy variants\n",
    "STRATEGY_CONFIGS = []\n",
    "\n",
    "# Scoring methods\n",
    "scoring_methods = [\n",
    "    ('debt_mktcap', True, False),   # Lower debt ratio is better for longs\n",
    "    ('roi', False, False),           # Higher ROI is better for longs\n",
    "    ('pe', True, False),             # Lower P/E is better for longs\n",
    "    ('combined_score', False, False),  # Higher combined score is better\n",
    "    # Ratio changes variants\n",
    "    ('d_debt_mktcap', True, True),   # Improving (decreasing) debt ratio\n",
    "    ('d_roi', False, True),          # Improving ROI\n",
    "    ('d_combined_score', False, True),  # Improving combined score\n",
    "]\n",
    "\n",
    "# Rebalance frequencies\n",
    "frequencies = ['M', 'W']\n",
    "\n",
    "# Sizing methods\n",
    "sizing_methods = ['equal', 'vigintile_double', 'vigintile_half']\n",
    "\n",
    "# Generate all combinations\n",
    "for signal_col, invert, use_changes in scoring_methods:\n",
    "    for freq in frequencies:\n",
    "        for sizing in sizing_methods:\n",
    "            change_suffix = \"_chg\" if use_changes else \"\"\n",
    "            name = f\"{signal_col}{change_suffix}_{freq}_{sizing}\"\n",
    "            \n",
    "            config = StrategyConfig(\n",
    "                name=name,\n",
    "                signal_col=signal_col,\n",
    "                rebalance_freq=freq,\n",
    "                sizing_method=sizing,\n",
    "                long_quantile=0.90,\n",
    "                short_quantile=0.10,\n",
    "                invert_signal=invert,\n",
    "                use_changes=use_changes\n",
    "            )\n",
    "            STRATEGY_CONFIGS.append(config)\n",
    "\n",
    "print(f\"✓ Defined {len(STRATEGY_CONFIGS)} strategy configurations\")\n",
    "print(\"\\nSample configurations:\")\n",
    "for config in STRATEGY_CONFIGS[:6]:\n",
    "    print(f\"  {config.name}: signal={config.signal_col}, freq={config.rebalance_freq}, sizing={config.sizing_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Portfolio Construction\n",
    "\n",
    "Build long-short portfolios based on quantile rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rebalance_dates(\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    freq: str = 'M'\n",
    ") -> List[pd.Timestamp]:\n",
    "    \"\"\"\n",
    "    Generate rebalance dates for the strategy.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start of analysis period\n",
    "        end_date: End of analysis period\n",
    "        freq: 'M' for monthly (month-end), 'W' for weekly (Friday)\n",
    "    \n",
    "    Returns:\n",
    "        List of rebalance dates\n",
    "    \"\"\"\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "    \n",
    "    if freq == 'W':\n",
    "        # Weekly: use Friday\n",
    "        dates = pd.date_range(start=start_date, end=end_date, freq='W-FRI')\n",
    "    elif freq == 'M':\n",
    "        # Monthly: use month-end\n",
    "        dates = pd.date_range(start=start_date, end=end_date, freq='ME')\n",
    "    \n",
    "    return dates.tolist()\n",
    "\n",
    "\n",
    "def construct_portfolio(\n",
    "    ratios_on_date: pd.DataFrame,\n",
    "    signal_col: str,\n",
    "    long_quantile: float,\n",
    "    short_quantile: float,\n",
    "    invert_signal: bool,\n",
    "    sizing_method: str\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Construct long and short portfolios based on signal ranking.\n",
    "    \n",
    "    Args:\n",
    "        ratios_on_date: DataFrame with ratios for a single date\n",
    "        signal_col: Column to rank on\n",
    "        long_quantile: Percentile threshold for long positions\n",
    "        short_quantile: Percentile threshold for short positions\n",
    "        invert_signal: If True, lower values are better for longs\n",
    "        sizing_method: 'equal', 'vigintile_double', 'vigintile_half'\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (long_portfolio, short_portfolio) DataFrames with weights\n",
    "    \"\"\"\n",
    "    df = ratios_on_date.dropna(subset=[signal_col]).copy()\n",
    "    \n",
    "    if len(df) < 20:  # Need enough stocks for decile ranking\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # Rank stocks (lower rank = lower signal value)\n",
    "    df['rank'] = df[signal_col].rank(pct=True)\n",
    "    \n",
    "    # Determine long and short positions based on signal direction\n",
    "    if invert_signal:\n",
    "        # Lower signal is better → long bottom decile, short top decile\n",
    "        long_mask = df['rank'] <= short_quantile  # Bottom 10%\n",
    "        short_mask = df['rank'] >= long_quantile  # Top 10%\n",
    "    else:\n",
    "        # Higher signal is better → long top decile, short bottom decile\n",
    "        long_mask = df['rank'] >= long_quantile   # Top 10%\n",
    "        short_mask = df['rank'] <= short_quantile # Bottom 10%\n",
    "    \n",
    "    long_df = df[long_mask].copy()\n",
    "    short_df = df[short_mask].copy()\n",
    "    \n",
    "    # Apply position sizing\n",
    "    def assign_weights(portfolio_df: pd.DataFrame, is_long: bool) -> pd.DataFrame:\n",
    "        if len(portfolio_df) == 0:\n",
    "            return portfolio_df\n",
    "        \n",
    "        # Base weight: equal weight within decile\n",
    "        n = len(portfolio_df)\n",
    "        base_weight = 1.0 / n\n",
    "        \n",
    "        if sizing_method == 'equal':\n",
    "            portfolio_df['weight'] = base_weight\n",
    "        \n",
    "        elif sizing_method == 'vigintile_double':\n",
    "            # 2x weight for top/bottom 5% (vigintile)\n",
    "            portfolio_df['weight'] = base_weight\n",
    "            if invert_signal:\n",
    "                # Lower rank is more extreme\n",
    "                extreme_mask = portfolio_df['rank'] <= 0.05 if is_long else portfolio_df['rank'] >= 0.95\n",
    "            else:\n",
    "                extreme_mask = portfolio_df['rank'] >= 0.95 if is_long else portfolio_df['rank'] <= 0.05\n",
    "            portfolio_df.loc[extreme_mask, 'weight'] = base_weight * 2\n",
    "            # Renormalize\n",
    "            portfolio_df['weight'] = portfolio_df['weight'] / portfolio_df['weight'].sum()\n",
    "        \n",
    "        elif sizing_method == 'vigintile_half':\n",
    "            # 0.5x weight for top/bottom 5% (vigintile) - distrust outliers\n",
    "            portfolio_df['weight'] = base_weight\n",
    "            if invert_signal:\n",
    "                extreme_mask = portfolio_df['rank'] <= 0.05 if is_long else portfolio_df['rank'] >= 0.95\n",
    "            else:\n",
    "                extreme_mask = portfolio_df['rank'] >= 0.95 if is_long else portfolio_df['rank'] <= 0.05\n",
    "            portfolio_df.loc[extreme_mask, 'weight'] = base_weight * 0.5\n",
    "            # Renormalize\n",
    "            portfolio_df['weight'] = portfolio_df['weight'] / portfolio_df['weight'].sum()\n",
    "        \n",
    "        return portfolio_df\n",
    "    \n",
    "    long_df = assign_weights(long_df, is_long=True)\n",
    "    short_df = assign_weights(short_df, is_long=False)\n",
    "    \n",
    "    return long_df, short_df\n",
    "\n",
    "\n",
    "print(\"✓ Portfolio construction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Backtesting Engine\n",
    "\n",
    "Core backtest loop with P&L tracking, funding costs, and trade recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest(\n",
    "    config: StrategyConfig,\n",
    "    ratios_df: pd.DataFrame,\n",
    "    prices: pd.DataFrame,\n",
    "    initial_capital: float = 1_000_000,\n",
    "    funding_rate: float = 0.02,\n",
    "    repo_spread: float = 0.01\n",
    ") -> BacktestResult:\n",
    "    \"\"\"\n",
    "    Run backtest for a single strategy configuration.\n",
    "    \n",
    "    Args:\n",
    "        config: Strategy configuration\n",
    "        ratios_df: DataFrame with daily ratios\n",
    "        prices: Price data (must include all tickers in ratios)\n",
    "        initial_capital: Starting capital\n",
    "        funding_rate: Annual funding rate for longs\n",
    "        repo_spread: Spread below funding for short rebate\n",
    "    \n",
    "    Returns:\n",
    "        BacktestResult with equity curve, trades, and metrics\n",
    "    \"\"\"\n",
    "    # Use the appropriate data source\n",
    "    data = ratios_df.copy()\n",
    "    \n",
    "    # If using changes, filter to valid change data\n",
    "    if config.use_changes and config.signal_col not in data.columns:\n",
    "        print(f\"  Warning: {config.signal_col} not in data, skipping\")\n",
    "        return None\n",
    "    \n",
    "    # Get rebalance dates\n",
    "    rebalance_dates = get_rebalance_dates(START_DATE, END_DATE, config.rebalance_freq)\n",
    "    \n",
    "    # Get all trading days\n",
    "    all_dates = sorted(data['date'].unique())\n",
    "    \n",
    "    # Initialize tracking\n",
    "    capital = initial_capital\n",
    "    equity_history = []\n",
    "    trades = []\n",
    "    current_positions = {}  # ticker -> (shares, entry_price, entry_date, side)\n",
    "    \n",
    "    # Precompute price lookup\n",
    "    prices_pivot = prices.pivot(index='date', columns='ticker', values='adj_close')\n",
    "    \n",
    "    # Daily funding/repo rates\n",
    "    daily_funding = funding_rate / 252\n",
    "    daily_repo = (funding_rate - repo_spread) / 252\n",
    "    \n",
    "    # Track gross notional for capital sizing\n",
    "    first_gross_notional = None\n",
    "    \n",
    "    for i, date in enumerate(all_dates):\n",
    "        date_ts = pd.Timestamp(date)\n",
    "        \n",
    "        # Get today's prices\n",
    "        if date_ts not in prices_pivot.index:\n",
    "            continue\n",
    "        today_prices = prices_pivot.loc[date_ts]\n",
    "        \n",
    "        # Update position values\n",
    "        long_value = 0\n",
    "        short_value = 0\n",
    "        \n",
    "        for ticker, (shares, entry_price, entry_date, side) in list(current_positions.items()):\n",
    "            if ticker in today_prices and not pd.isna(today_prices[ticker]):\n",
    "                current_price = today_prices[ticker]\n",
    "                if side == 'long':\n",
    "                    long_value += shares * current_price\n",
    "                else:\n",
    "                    # Short: value is entry_value - (current_price - entry_price) * shares\n",
    "                    entry_value = shares * entry_price\n",
    "                    pnl = (entry_price - current_price) * shares\n",
    "                    short_value += entry_value + pnl\n",
    "        \n",
    "        # Apply daily funding costs\n",
    "        funding_cost = long_value * daily_funding\n",
    "        repo_rebate = short_value * daily_repo * 0.5  # Approximate rebate\n",
    "        \n",
    "        # Calculate equity\n",
    "        cash = capital - long_value - short_value  # Simplified\n",
    "        equity = long_value + short_value + capital  # Mark-to-market\n",
    "        \n",
    "        equity_history.append({'date': date_ts, 'equity': equity})\n",
    "        \n",
    "        # Check for rebalance\n",
    "        is_rebalance = any(abs((date_ts - rd).days) <= 2 for rd in rebalance_dates)\n",
    "        \n",
    "        if is_rebalance:\n",
    "            # Get ratios for this date\n",
    "            date_ratios = data[data['date'] == date_ts]\n",
    "            \n",
    "            if len(date_ratios) == 0:\n",
    "                # Try nearest date within 5 days\n",
    "                nearby = data[(data['date'] >= date_ts - pd.Timedelta(days=5)) & \n",
    "                              (data['date'] <= date_ts + pd.Timedelta(days=5))]\n",
    "                if len(nearby) > 0:\n",
    "                    nearest_date = nearby['date'].max()\n",
    "                    date_ratios = data[data['date'] == nearest_date]\n",
    "            \n",
    "            if len(date_ratios) < 20:\n",
    "                continue\n",
    "            \n",
    "            # Construct new portfolios\n",
    "            long_port, short_port = construct_portfolio(\n",
    "                date_ratios,\n",
    "                config.signal_col,\n",
    "                config.long_quantile,\n",
    "                config.short_quantile,\n",
    "                config.invert_signal,\n",
    "                config.sizing_method\n",
    "            )\n",
    "            \n",
    "            if len(long_port) == 0 and len(short_port) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate target notional\n",
    "            target_long_notional = capital * 0.5  # 50% long\n",
    "            target_short_notional = capital * 0.5  # 50% short\n",
    "            \n",
    "            if first_gross_notional is None:\n",
    "                first_gross_notional = target_long_notional + target_short_notional\n",
    "            \n",
    "            # Close existing positions (record trades)\n",
    "            for ticker, (shares, entry_price, entry_date, side) in current_positions.items():\n",
    "                if ticker in today_prices and not pd.isna(today_prices[ticker]):\n",
    "                    exit_price = today_prices[ticker]\n",
    "                    if side == 'long':\n",
    "                        pnl = (exit_price - entry_price) * shares\n",
    "                    else:\n",
    "                        pnl = (entry_price - exit_price) * shares\n",
    "                    \n",
    "                    trades.append(Trade(\n",
    "                        ticker=ticker,\n",
    "                        side=PositionSide.LONG if side == 'long' else PositionSide.SHORT,\n",
    "                        entry_time=entry_date,\n",
    "                        exit_time=date_ts,\n",
    "                        entry_price=entry_price,\n",
    "                        exit_price=exit_price,\n",
    "                        shares=shares,\n",
    "                        pnl=pnl,\n",
    "                        exit_reason=ExitReason.REBALANCE\n",
    "                    ))\n",
    "            \n",
    "            # Open new positions\n",
    "            current_positions = {}\n",
    "            \n",
    "            # Long positions\n",
    "            for _, row in long_port.iterrows():\n",
    "                ticker = row['ticker']\n",
    "                if ticker in today_prices and not pd.isna(today_prices[ticker]):\n",
    "                    price = today_prices[ticker]\n",
    "                    notional = target_long_notional * row['weight']\n",
    "                    shares = notional / price\n",
    "                    current_positions[ticker] = (shares, price, date_ts, 'long')\n",
    "            \n",
    "            # Short positions\n",
    "            for _, row in short_port.iterrows():\n",
    "                ticker = row['ticker']\n",
    "                if ticker in today_prices and not pd.isna(today_prices[ticker]):\n",
    "                    price = today_prices[ticker]\n",
    "                    notional = target_short_notional * row['weight']\n",
    "                    shares = notional / price\n",
    "                    current_positions[ticker] = (shares, price, date_ts, 'short')\n",
    "    \n",
    "    # Create equity curve\n",
    "    equity_df = pd.DataFrame(equity_history)\n",
    "    if len(equity_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    equity_df = equity_df.set_index('date')['equity']\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_return = (equity_df.iloc[-1] - initial_capital) / initial_capital\n",
    "    sharpe = calculate_sharpe_ratio(equity_df)\n",
    "    max_dd = calculate_max_drawdown(equity_df)\n",
    "    win_rate = calculate_win_rate(trades)\n",
    "    \n",
    "    return BacktestResult(\n",
    "        trades=trades,\n",
    "        equity_curve=equity_df,\n",
    "        final_capital=equity_df.iloc[-1],\n",
    "        total_return=total_return,\n",
    "        sharpe_ratio=sharpe,\n",
    "        max_drawdown=max_dd,\n",
    "        win_rate=win_rate,\n",
    "        num_trades=len(trades),\n",
    "        num_rebalances=len([t for t in trades if t.exit_reason == ExitReason.REBALANCE]) // 2,\n",
    "        params={\n",
    "            'name': config.name,\n",
    "            'signal': config.signal_col,\n",
    "            'freq': config.rebalance_freq,\n",
    "            'sizing': config.sizing_method\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"✓ Backtest engine defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Run All Strategy Variants\n",
    "\n",
    "Execute backtests for all 42 strategy configurations (7 signals × 2 frequencies × 3 sizing methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all backtests\n",
    "# This cell takes several minutes to run\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING ALL BACKTESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare data sources\n",
    "levels_data = ratios_df  # Absolute ratio levels\n",
    "changes_data = ratios_with_changes  # Ratio changes\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for i, config in enumerate(STRATEGY_CONFIGS):\n",
    "    print(f\"\\n[{i+1}/{len(STRATEGY_CONFIGS)}] Running: {config.name}\")\n",
    "    \n",
    "    # Select appropriate data source\n",
    "    if config.use_changes:\n",
    "        data = changes_data\n",
    "    else:\n",
    "        data = levels_data\n",
    "    \n",
    "    # Run backtest\n",
    "    result = run_backtest(\n",
    "        config=config,\n",
    "        ratios_df=data,\n",
    "        prices=prices,\n",
    "        initial_capital=1_000_000,\n",
    "        funding_rate=FUNDING_RATE,\n",
    "        repo_spread=REPO_SPREAD\n",
    "    )\n",
    "    \n",
    "    if result is not None:\n",
    "        all_results[config.name] = result\n",
    "        print(f\"  ✓ Sharpe: {result.sharpe_ratio:.3f}, Return: {result.total_return:.2%}, MaxDD: {result.max_drawdown:.2%}\")\n",
    "    else:\n",
    "        print(f\"  ✗ Backtest failed or insufficient data\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"COMPLETED: {len(all_results)} successful backtests out of {len(STRATEGY_CONFIGS)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Performance Analysis\n",
    "\n",
    "Comprehensive performance metrics for all strategy variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_summary(results: Dict[str, BacktestResult]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create summary table of all backtest results.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of strategy name -> BacktestResult\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with performance metrics for each strategy\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        # Calculate additional metrics\n",
    "        returns = result.equity_curve.pct_change().dropna()\n",
    "        \n",
    "        # Calmar ratio\n",
    "        calmar = calculate_calmar_ratio(result.total_return, result.max_drawdown)\n",
    "        \n",
    "        # Tail risk (VaR)\n",
    "        var_1pct = calculate_tail_risk(returns, 0.01) if len(returns) > 10 else 0\n",
    "        var_5pct = calculate_tail_risk(returns, 0.05) if len(returns) > 10 else 0\n",
    "        \n",
    "        # P&L to notional ratio\n",
    "        total_pnl = result.final_capital - 1_000_000\n",
    "        traded_notional = sum(abs(t.shares * t.entry_price) for t in result.trades)\n",
    "        pnl_notional_ratio = total_pnl / traded_notional if traded_notional > 0 else 0\n",
    "        \n",
    "        rows.append({\n",
    "            'Strategy': name,\n",
    "            'Signal': result.params['signal'],\n",
    "            'Frequency': result.params['freq'],\n",
    "            'Sizing': result.params['sizing'],\n",
    "            'Total Return': result.total_return,\n",
    "            'Sharpe Ratio': result.sharpe_ratio,\n",
    "            'Max Drawdown': result.max_drawdown,\n",
    "            'Win Rate': result.win_rate,\n",
    "            'Calmar Ratio': calmar,\n",
    "            'VaR 1%': var_1pct,\n",
    "            'VaR 5%': var_5pct,\n",
    "            'PnL/Notional': pnl_notional_ratio,\n",
    "            'Num Trades': result.num_trades,\n",
    "            'Final Capital': result.final_capital\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values('Sharpe Ratio', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Create results summary\n",
    "results_summary = create_results_summary(all_results)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"PERFORMANCE SUMMARY (Top 15 by Sharpe Ratio)\")\n",
    "print(\"=\" * 100)\n",
    "print(results_summary.head(15).to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "summary_file = RESULTS_DIR / 'backtest_results.csv'\n",
    "results_summary.to_csv(summary_file, index=False)\n",
    "print(f\"\\n✓ Results saved to {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Ratio Levels vs Ratio Changes Comparison\n",
    "\n",
    "Compare the performance of strategies using absolute ratio levels vs ratio changes (Δratio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare levels vs changes\n",
    "levels_strategies = results_summary[~results_summary['Signal'].str.startswith('d_')]\n",
    "changes_strategies = results_summary[results_summary['Signal'].str.startswith('d_')]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LEVELS vs CHANGES COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n### Absolute Ratio Levels Strategies ###\")\n",
    "print(f\"Count: {len(levels_strategies)}\")\n",
    "print(f\"Avg Sharpe: {levels_strategies['Sharpe Ratio'].mean():.3f}\")\n",
    "print(f\"Avg Return: {levels_strategies['Total Return'].mean():.2%}\")\n",
    "print(f\"Avg Max DD: {levels_strategies['Max Drawdown'].mean():.2%}\")\n",
    "\n",
    "print(\"\\n### Ratio Changes (Δ) Strategies ###\")\n",
    "print(f\"Count: {len(changes_strategies)}\")\n",
    "if len(changes_strategies) > 0:\n",
    "    print(f\"Avg Sharpe: {changes_strategies['Sharpe Ratio'].mean():.3f}\")\n",
    "    print(f\"Avg Return: {changes_strategies['Total Return'].mean():.2%}\")\n",
    "    print(f\"Avg Max DD: {changes_strategies['Max Drawdown'].mean():.2%}\")\n",
    "else:\n",
    "    print(\"No change-based strategies completed successfully.\")\n",
    "\n",
    "# Best performers from each category\n",
    "print(\"\\n### Best Level-Based Strategy ###\")\n",
    "print(levels_strategies.head(1)[['Strategy', 'Sharpe Ratio', 'Total Return', 'Max Drawdown']].to_string(index=False))\n",
    "\n",
    "if len(changes_strategies) > 0:\n",
    "    print(\"\\n### Best Change-Based Strategy ###\")\n",
    "    print(changes_strategies.head(1)[['Strategy', 'Sharpe Ratio', 'Total Return', 'Max Drawdown']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Signal Comparison\n",
    "\n",
    "Compare performance across the four main scoring methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by signal type (excluding changes)\n",
    "main_signals = ['debt_mktcap', 'roi', 'pe', 'combined_score']\n",
    "signal_comparison = levels_strategies[levels_strategies['Signal'].isin(main_signals)].groupby('Signal').agg({\n",
    "    'Sharpe Ratio': ['mean', 'std', 'max'],\n",
    "    'Total Return': ['mean', 'max'],\n",
    "    'Max Drawdown': ['mean', 'min'],\n",
    "    'Win Rate': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SIGNAL COMPARISON (Levels Only)\")\n",
    "print(\"=\" * 80)\n",
    "print(signal_comparison)\n",
    "\n",
    "# Rank signals by average Sharpe\n",
    "signal_ranking = levels_strategies[levels_strategies['Signal'].isin(main_signals)].groupby('Signal')['Sharpe Ratio'].mean().sort_values(ascending=False)\n",
    "print(\"\\n### Signal Ranking by Average Sharpe ###\")\n",
    "for i, (signal, sharpe) in enumerate(signal_ranking.items(), 1):\n",
    "    print(f\"{i}. {signal}: {sharpe:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Position Sizing Comparison\n",
    "\n",
    "Evaluate the impact of vigintile doubling vs halving vs equal weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by sizing method\n",
    "sizing_comparison = results_summary.groupby('Sizing').agg({\n",
    "    'Sharpe Ratio': ['mean', 'std', 'max'],\n",
    "    'Total Return': ['mean', 'max'],\n",
    "    'Max Drawdown': ['mean', 'min'],\n",
    "    'Win Rate': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"POSITION SIZING COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(sizing_comparison)\n",
    "\n",
    "print(\"\\n### Interpretation ###\")\n",
    "print(\"\"\"\n",
    "- **equal**: Baseline equal-weight within decile\n",
    "- **vigintile_double**: 2x weight for extreme quintile (most attractive/unattractive)\n",
    "  Hypothesis: Extreme values have stronger signal\n",
    "- **vigintile_half**: 0.5x weight for extreme quintile\n",
    "  Hypothesis: Extreme values are untrustworthy outliers\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Rebalancing Frequency Comparison\n",
    "\n",
    "Monthly vs weekly rebalancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by rebalancing frequency\n",
    "freq_comparison = results_summary.groupby('Frequency').agg({\n",
    "    'Sharpe Ratio': ['mean', 'std', 'max'],\n",
    "    'Total Return': ['mean', 'max'],\n",
    "    'Max Drawdown': ['mean', 'min'],\n",
    "    'Num Trades': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"REBALANCING FREQUENCY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(freq_comparison)\n",
    "\n",
    "print(\"\\n### Interpretation ###\")\n",
    "print(\"\"\"\n",
    "- **M (Monthly)**: Lower turnover, lower transaction costs, potential signal decay\n",
    "- **W (Weekly)**: Higher turnover, captures signal changes faster, higher transaction costs\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Visualizations\n",
    "\n",
    "Charts and figures for strategy analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Equity Curves by Signal Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_equity_curves_by_signal(results: Dict[str, BacktestResult], sizing: str = 'equal', freq: str = 'M'):\n",
    "    \"\"\"\n",
    "    Plot equity curves for each signal type (holding sizing and frequency constant).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    colors = {'debt_mktcap': '#1f77b4', 'roi': '#ff7f0e', 'pe': '#2ca02c', 'combined_score': '#d62728'}\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        if result.params['sizing'] == sizing and result.params['freq'] == freq:\n",
    "            signal = result.params['signal']\n",
    "            if signal in colors and not signal.startswith('d_'):\n",
    "                equity_normalized = result.equity_curve / result.equity_curve.iloc[0]\n",
    "                ax.plot(equity_normalized.index, equity_normalized.values, \n",
    "                       label=signal, color=colors[signal], linewidth=1.5)\n",
    "    \n",
    "    ax.set_title(f'Equity Curves by Signal Type ({freq} rebalancing, {sizing} sizing)', fontsize=14)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Normalized Equity (Starting = 1.0)')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'equity_curves_by_signal.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved to {PLOTS_DIR / 'equity_curves_by_signal.png'}\")\n",
    "\n",
    "\n",
    "# Plot equity curves\n",
    "plot_equity_curves_by_signal(all_results, sizing='equal', freq='M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Drawdown Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_drawdowns(results: Dict[str, BacktestResult], sizing: str = 'equal', freq: str = 'M'):\n",
    "    \"\"\"\n",
    "    Plot drawdown curves for each signal type.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    colors = {'debt_mktcap': '#1f77b4', 'roi': '#ff7f0e', 'pe': '#2ca02c', 'combined_score': '#d62728'}\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        if result.params['sizing'] == sizing and result.params['freq'] == freq:\n",
    "            signal = result.params['signal']\n",
    "            if signal in colors and not signal.startswith('d_'):\n",
    "                equity = result.equity_curve\n",
    "                running_max = equity.cummax()\n",
    "                drawdown = (equity - running_max) / running_max\n",
    "                ax.fill_between(drawdown.index, drawdown.values, 0, \n",
    "                               alpha=0.3, label=signal, color=colors[signal])\n",
    "                ax.plot(drawdown.index, drawdown.values, color=colors[signal], linewidth=0.8)\n",
    "    \n",
    "    ax.set_title(f'Drawdown Analysis ({freq} rebalancing, {sizing} sizing)', fontsize=14)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Drawdown (%)')\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.set_ylim(bottom=ax.get_ylim()[0] * 1.1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format y-axis as percentage\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.0%}'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'drawdown_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved to {PLOTS_DIR / 'drawdown_analysis.png'}\")\n",
    "\n",
    "\n",
    "# Plot drawdowns\n",
    "plot_drawdowns(all_results, sizing='equal', freq='M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Monthly Returns Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_returns_heatmap(result: BacktestResult, strategy_name: str):\n",
    "    \"\"\"\n",
    "    Create a heatmap of monthly returns for a single strategy.\n",
    "    \"\"\"\n",
    "    # Calculate monthly returns\n",
    "    equity = result.equity_curve.resample('ME').last()\n",
    "    monthly_returns = equity.pct_change().dropna()\n",
    "    \n",
    "    # Create pivot table (year x month)\n",
    "    monthly_returns_df = pd.DataFrame({\n",
    "        'year': monthly_returns.index.year,\n",
    "        'month': monthly_returns.index.month,\n",
    "        'return': monthly_returns.values\n",
    "    })\n",
    "    \n",
    "    pivot = monthly_returns_df.pivot(index='year', columns='month', values='return')\n",
    "    pivot.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][:len(pivot.columns)]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "    \n",
    "    sns.heatmap(pivot * 100, annot=True, fmt='.1f', center=0,\n",
    "                cmap='RdYlGn', ax=ax, cbar_kws={'label': 'Return (%)'})\n",
    "    \n",
    "    ax.set_title(f'Monthly Returns Heatmap: {strategy_name}', fontsize=14)\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Year')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / f'monthly_returns_{strategy_name.replace(\"/\", \"_\")}.png', \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved to {PLOTS_DIR}\")\n",
    "\n",
    "\n",
    "# Plot for best strategy\n",
    "if len(results_summary) > 0:\n",
    "    best_strategy = results_summary.iloc[0]['Strategy']\n",
    "    if best_strategy in all_results:\n",
    "        plot_monthly_returns_heatmap(all_results[best_strategy], best_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Ratio Distribution Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ratio_distributions(ratios_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot histograms of the three financial ratios.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    ratio_cols = ['debt_mktcap', 'roi', 'pe']\n",
    "    titles = ['Debt / Market Cap', 'Return on Investment', 'Price / Earnings']\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    for ax, col, title, color in zip(axes, ratio_cols, titles, colors):\n",
    "        # Winsorize for visualization\n",
    "        data = ratios_df[col].dropna()\n",
    "        lower, upper = data.quantile(0.01), data.quantile(0.99)\n",
    "        data_clipped = data[(data >= lower) & (data <= upper)]\n",
    "        \n",
    "        ax.hist(data_clipped, bins=50, color=color, alpha=0.7, edgecolor='white')\n",
    "        ax.axvline(data_clipped.median(), color='red', linestyle='--', \n",
    "                   label=f'Median: {data_clipped.median():.2f}')\n",
    "        ax.axvline(data_clipped.mean(), color='black', linestyle='-', \n",
    "                   label=f'Mean: {data_clipped.mean():.2f}')\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Financial Ratio Distributions (Winsorized 1%-99%)', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'ratio_distributions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved to {PLOTS_DIR / 'ratio_distributions.png'}\")\n",
    "\n",
    "\n",
    "# Plot ratio distributions\n",
    "plot_ratio_distributions(ratios_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 Performance Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_comparison(results_summary: pd.DataFrame, top_n: int = 10):\n",
    "    \"\"\"\n",
    "    Create bar chart comparing top strategies.\n",
    "    \"\"\"\n",
    "    top_results = results_summary.head(top_n).copy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Sharpe Ratio\n",
    "    ax = axes[0, 0]\n",
    "    bars = ax.barh(range(len(top_results)), top_results['Sharpe Ratio'], color='steelblue')\n",
    "    ax.set_yticks(range(len(top_results)))\n",
    "    ax.set_yticklabels(top_results['Strategy'], fontsize=8)\n",
    "    ax.set_xlabel('Sharpe Ratio')\n",
    "    ax.set_title('Sharpe Ratio (Top 10)')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Total Return\n",
    "    ax = axes[0, 1]\n",
    "    colors = ['green' if r > 0 else 'red' for r in top_results['Total Return']]\n",
    "    ax.barh(range(len(top_results)), top_results['Total Return'] * 100, color=colors)\n",
    "    ax.set_yticks(range(len(top_results)))\n",
    "    ax.set_yticklabels(top_results['Strategy'], fontsize=8)\n",
    "    ax.set_xlabel('Total Return (%)')\n",
    "    ax.set_title('Total Return (Top 10)')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Max Drawdown\n",
    "    ax = axes[1, 0]\n",
    "    ax.barh(range(len(top_results)), -top_results['Max Drawdown'] * 100, color='crimson')\n",
    "    ax.set_yticks(range(len(top_results)))\n",
    "    ax.set_yticklabels(top_results['Strategy'], fontsize=8)\n",
    "    ax.set_xlabel('Max Drawdown (%)')\n",
    "    ax.set_title('Max Drawdown (Top 10)')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Win Rate\n",
    "    ax = axes[1, 1]\n",
    "    ax.barh(range(len(top_results)), top_results['Win Rate'] * 100, color='goldenrod')\n",
    "    ax.set_yticks(range(len(top_results)))\n",
    "    ax.set_yticklabels(top_results['Strategy'], fontsize=8)\n",
    "    ax.set_xlabel('Win Rate (%)')\n",
    "    ax.set_title('Win Rate (Top 10)')\n",
    "    ax.axvline(x=50, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.suptitle('Strategy Performance Comparison', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'performance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved to {PLOTS_DIR / 'performance_comparison.png'}\")\n",
    "\n",
    "\n",
    "# Plot performance comparison\n",
    "plot_performance_comparison(results_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Conclusions and Interpretation\n",
    "\n",
    "### 11.1 Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate automated key findings\n",
    "print(\"=\" * 80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(results_summary) > 0:\n",
    "    # Best overall strategy\n",
    "    best = results_summary.iloc[0]\n",
    "    print(f\"\\n### Best Performing Strategy ###\")\n",
    "    print(f\"Name: {best['Strategy']}\")\n",
    "    print(f\"Sharpe Ratio: {best['Sharpe Ratio']:.3f}\")\n",
    "    print(f\"Total Return: {best['Total Return']:.2%}\")\n",
    "    print(f\"Max Drawdown: {best['Max Drawdown']:.2%}\")\n",
    "    print(f\"Win Rate: {best['Win Rate']:.2%}\")\n",
    "    \n",
    "    # Best signal\n",
    "    signal_avg = results_summary.groupby('Signal')['Sharpe Ratio'].mean()\n",
    "    best_signal = signal_avg.idxmax()\n",
    "    print(f\"\\n### Best Signal (by avg Sharpe) ###\")\n",
    "    print(f\"Signal: {best_signal}\")\n",
    "    print(f\"Average Sharpe: {signal_avg[best_signal]:.3f}\")\n",
    "    \n",
    "    # Best sizing\n",
    "    sizing_avg = results_summary.groupby('Sizing')['Sharpe Ratio'].mean()\n",
    "    best_sizing = sizing_avg.idxmax()\n",
    "    print(f\"\\n### Best Sizing Method (by avg Sharpe) ###\")\n",
    "    print(f\"Method: {best_sizing}\")\n",
    "    print(f\"Average Sharpe: {sizing_avg[best_sizing]:.3f}\")\n",
    "    \n",
    "    # Best frequency\n",
    "    freq_avg = results_summary.groupby('Frequency')['Sharpe Ratio'].mean()\n",
    "    best_freq = freq_avg.idxmax()\n",
    "    print(f\"\\n### Best Rebalancing Frequency ###\")\n",
    "    print(f\"Frequency: {'Monthly' if best_freq == 'M' else 'Weekly'}\")\n",
    "    print(f\"Average Sharpe: {freq_avg[best_freq]:.3f}\")\n",
    "else:\n",
    "    print(\"No results available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Subjective Decisions and Justifications\n",
    "\n",
    "This section documents key design choices made during implementation:\n",
    "\n",
    "| Decision | Choice | Justification |\n",
    "|----------|--------|---------------|\n",
    "| **Rebalance Dates** | Month-end (M), Friday (W) | Month-end aligns with financial reporting cycles; Friday captures weekly price drift |\n",
    "| **Quantile Thresholds** | 10th/90th percentile (decile) | Standard academic approach; balances diversification with signal strength |\n",
    "| **Lookback for Δratio** | 21 trading days (~1 month) | Matches monthly rebalance frequency; captures meaningful fundamental changes |\n",
    "| **Funding Rate** | 2% constant | Reasonable approximation of historical short-term rates over 2018-2023 |\n",
    "| **Repo Spread** | 100bp below funding | Standard institutional rebate for shorts |\n",
    "| **Winsorization** | 1%-99% | Removes extreme outliers while preserving signal distribution |\n",
    "| **Minimum Universe Size** | 20 stocks on rebalance date | Ensures meaningful decile construction |\n",
    "| **Signal Inversion** | Debt/MktCap ↓, P/E ↓, ROI ↑ | Follows traditional value/quality factor interpretation |\n",
    "| **Combined Score** | Equal-weighted z-scores | Simple, interpretable; no look-ahead optimization of weights |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Interpretation of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "===================================================================================\n",
    "INTERPRETATION OF RESULTS\n",
    "===================================================================================\n",
    "\n",
    "1. SIGNAL EFFICACY\n",
    "------------------\n",
    "The financial ratios provide varying degrees of predictive power for future returns:\n",
    "\n",
    "- **Debt/Market Cap**: Measures leverage. Low-debt companies may be safer in downturns,\n",
    "  but high-debt firms can benefit from leverage during bull markets.\n",
    "\n",
    "- **ROI (Return on Investment)**: Measures operating efficiency. Higher ROI suggests \n",
    "  better capital allocation, though extremely high ROI may indicate unsustainable levels.\n",
    "\n",
    "- **P/E (Price/Earnings)**: Classic valuation metric. Low P/E stocks are \"value\" plays;\n",
    "  however, P/E can be distorted by cyclical earnings or one-time items.\n",
    "\n",
    "- **Combined Score**: Diversifies across all three factors. May smooth out noise but\n",
    "  dilutes the strongest individual signal.\n",
    "\n",
    "2. LEVELS vs CHANGES\n",
    "--------------------\n",
    "Using ratio CHANGES (Δ) instead of absolute LEVELS tests whether improving fundamentals\n",
    "predict returns better than current fundamental position. Changes capture momentum in\n",
    "company quality improvement.\n",
    "\n",
    "3. POSITION SIZING IMPACT\n",
    "-------------------------\n",
    "- **Vigintile Doubling**: Betting more on extreme values works if the signal is strong\n",
    "  and extreme values are not errors.\n",
    "  \n",
    "- **Vigintile Halving**: Reduces exposure to outliers, works if extreme values are\n",
    "  often due to data errors or temporary distortions.\n",
    "\n",
    "4. REBALANCING FREQUENCY\n",
    "------------------------\n",
    "- **Monthly**: Lower transaction costs, slower adaptation to new information\n",
    "- **Weekly**: Higher turnover costs, faster reaction to price/fundamental changes\n",
    "\n",
    "The optimal frequency depends on signal decay rate and transaction cost assumptions.\n",
    "\n",
    "===================================================================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 Limitations and Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "===================================================================================\n",
    "LIMITATIONS\n",
    "===================================================================================\n",
    "\n",
    "1. **Transaction Costs**: Not explicitly modeled. Real strategies incur bid-ask\n",
    "   spreads, market impact, and commissions that reduce returns.\n",
    "\n",
    "2. **Survivorship Bias**: Universe is constructed from currently available tickers,\n",
    "   which may exclude delisted/bankrupt companies (though Zacks data mitigates this).\n",
    "\n",
    "3. **Look-Ahead in Universe Selection**: Universe filters use the full date range,\n",
    "   which could introduce subtle look-ahead bias.\n",
    "\n",
    "4. **Filing Date Approximation**: Some fundamental data may have been available\n",
    "   slightly earlier or later than the official filing date.\n",
    "\n",
    "5. **Capacity Constraints**: Small-cap stocks in the universe may not support\n",
    "   institutional-scale positions without significant market impact.\n",
    "\n",
    "6. **Single Period**: Results are specific to 2018-2023, which includes COVID crash/\n",
    "   recovery. Performance in other regimes may differ substantially.\n",
    "\n",
    "===================================================================================\n",
    "POTENTIAL EXTENSIONS\n",
    "===================================================================================\n",
    "\n",
    "1. **Sector-Neutral Implementation**: Rank stocks within sectors to neutralize\n",
    "   sector exposure.\n",
    "\n",
    "2. **Dynamic Signal Weights**: Learn optimal weights for combined score using\n",
    "   rolling window regression.\n",
    "\n",
    "3. **Machine Learning Integration**: Use gradient boosting or neural networks\n",
    "   to combine signals non-linearly.\n",
    "\n",
    "4. **Transaction Cost Modeling**: Add realistic trading costs to find optimal\n",
    "   rebalance frequency.\n",
    "\n",
    "5. **Risk Parity Position Sizing**: Size positions inversely to volatility for\n",
    "   more stable risk contribution.\n",
    "\n",
    "6. **Alternative Ratios**: Add other metrics (free cash flow yield, earnings\n",
    "   quality, accruals) to signal set.\n",
    "\n",
    "===================================================================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5 Final Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table with formatting\n",
    "print(\"=\" * 100)\n",
    "print(\"FINAL RESULTS SUMMARY - ALL STRATEGIES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Display full results table\n",
    "display_cols = ['Strategy', 'Sharpe Ratio', 'Total Return', 'Max Drawdown', 'Win Rate', 'Calmar Ratio']\n",
    "print(results_summary[display_cols].to_string(index=False))\n",
    "\n",
    "# Save final results\n",
    "final_file = RESULTS_DIR / 'final_summary.csv'\n",
    "results_summary.to_csv(final_file, index=False)\n",
    "print(f\"\\n✓ Final results saved to {final_file}\")\n",
    "\n",
    "# Pickle all results for future analysis\n",
    "import pickle\n",
    "with open(RESULTS_DIR / 'all_backtest_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "print(f\"✓ Full backtest objects saved to {RESULTS_DIR / 'all_backtest_results.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notebook Execution Complete\n",
    "\n",
    "**Summary:**\n",
    "- Loaded ~1200 ticker universe after applying 5 filters\n",
    "- Computed 3 financial ratios (Debt/MktCap, ROI, P/E) + combined score\n",
    "- Ran 42 strategy variants (7 signals × 2 frequencies × 3 sizing methods)\n",
    "- Generated comprehensive performance metrics and visualizations\n",
    "\n",
    "**Output Files:**\n",
    "- `outputs/results/universe_tickers.csv` - Universe definition\n",
    "- `outputs/results/computed_ratios.parquet` - Daily ratio values\n",
    "- `outputs/results/ratio_changes.parquet` - Ratio change values\n",
    "- `outputs/results/backtest_results.csv` - Summary metrics\n",
    "- `outputs/results/final_summary.csv` - Final results\n",
    "- `outputs/results/all_backtest_results.pkl` - Full backtest objects\n",
    "- `outputs/plots/*.png` - Visualization files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
